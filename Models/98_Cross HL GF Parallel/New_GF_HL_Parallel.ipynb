{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import sys\n",
    "sys.path.append(\"./../\")\n",
    "import os\n",
    "import numpy as np\n",
    "import random\n",
    "from torch import einsum\n",
    "import torch.nn.functional as F\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as dataf\n",
    "from torch.utils.data import Dataset\n",
    "from scipy import io\n",
    "from scipy.io import loadmat as loadmat\n",
    "from sklearn.decomposition import PCA\n",
    "from torch.nn.parameter import Parameter\n",
    "import torchvision.transforms.functional as TF\n",
    "from torch.nn import LayerNorm,Linear,Dropout,Softmax\n",
    "import time\n",
    "from PIL import Image\n",
    "import math\n",
    "from operator import truediv\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, classification_report, cohen_kappa_score\n",
    "from torchsummary import summary\n",
    "import matplotlib.pyplot as plt\n",
    "import torch.backends.cudnn as cudnn\n",
    "import re\n",
    "from pathlib import Path\n",
    "import copy\n",
    "\n",
    "import utils\n",
    "import logger\n",
    "\n",
    "cudnn.deterministic = True\n",
    "cudnn.benchmark = False\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Dataset\n",
    "class HSI_LiDAR_DatasetTrain(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset='Trento'):\n",
    "\n",
    "        HSI = loadmat(f'./{dataset}11x11/HSI_Tr.mat')\n",
    "        LiDAR = loadmat(f'./{dataset}11x11/LIDAR_Tr.mat')\n",
    "        label = loadmat(f'./{dataset}11x11/TrLabel.mat')\n",
    "\n",
    "        self.hs_image = (torch.from_numpy(HSI['Data'].astype(np.float32)).to(torch.float32)).permute(0,3,1,2)\n",
    "        self.lidar_image = (torch.from_numpy(LiDAR['Data'].astype(np.float32)).to(torch.float32)).permute(0,3,1,2)\n",
    "        self.lbls = ((torch.from_numpy(label['Data'])-1).long()).reshape(-1)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.hs_image.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.hs_image[i], self.lidar_image[i], self.lbls[i]\n",
    "\n",
    "class HSI_LiDAR_DatasetTest(torch.utils.data.Dataset):\n",
    "    def __init__(self, dataset='Trento'):\n",
    "\n",
    "        HSI = loadmat(f'./{dataset}11x11/HSI_Te.mat')\n",
    "        LiDAR = loadmat(f'./{dataset}11x11/LIDAR_Te.mat')\n",
    "        label = loadmat(f'./{dataset}11x11/TeLabel.mat')\n",
    "\n",
    "        self.hs_image = (torch.from_numpy(HSI['Data'].astype(np.float32)).to(torch.float32)).permute(0,3,1,2)\n",
    "        self.lidar_image = (torch.from_numpy(LiDAR['Data'].astype(np.float32)).to(torch.float32)).permute(0,3,1,2)\n",
    "        self.lbls = ((torch.from_numpy(label['Data'])-1).long()).reshape(-1)\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.hs_image.shape[0]\n",
    "\n",
    "    def __getitem__(self, i):\n",
    "        return self.hs_image[i], self.lidar_image[i], self.lbls[i]\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "class HetConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1,padding = None, bias = None,p = 64, g = 64):\n",
    "        super(HetConv, self).__init__()\n",
    "        # Groupwise Convolution\n",
    "        self.groupwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size,groups=g,padding = kernel_size//3, stride = stride)\n",
    "        # Pointwise Convolution\n",
    "        self.pointwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1,groups=p, stride = stride)\n",
    "    def forward(self, x):\n",
    "        return self.groupwise_conv(x) + self.pointwise_conv(x)\n",
    "\n",
    "# Cross-HL Attention Module\n",
    "class CrossHL_attention(nn.Module):\n",
    "    def __init__(self, dim, patches, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.1, proj_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.dim = dim\n",
    "        self.Wq = nn.Linear(patches, dim * num_heads , bias=qkv_bias)\n",
    "        self.Wk = nn.Linear(dim, dim , bias=qkv_bias)\n",
    "        self.Wv = nn.Linear(patches+1, dim , bias=qkv_bias)\n",
    "        self.linear_projection = nn.Linear(dim * num_heads, dim)\n",
    "        self.linear_projection_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x, x2):\n",
    "\n",
    "        B, N, C = x.shape\n",
    "        # query vector using lidar data\n",
    "        query = self.Wq(x2).reshape(B, self.num_heads, self.num_heads, self.dim // self.num_heads).permute(0, 1, 2, 3)\n",
    "\n",
    "        key = self.Wk(x).reshape(B, N, self.num_heads, self.dim // self.num_heads).permute(0, 2, 1, 3)\n",
    "        value = self.Wv(x.transpose(1,2)).reshape(B, C, self.num_heads, self.dim // self.num_heads).permute(0, 2, 3, 1)\n",
    "        attention = torch.einsum('bhid,bhjd->bhij', key, query) * self.scale\n",
    "        attention = attention.softmax(dim=-1)\n",
    "\n",
    "        x = torch.einsum('bhij,bhjd->bhid', attention, value)\n",
    "        x = x.reshape(B, N, -1)\n",
    "        x = self.linear_projection(x)\n",
    "        x = self.linear_projection_drop(x)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class GlobalFilter(nn.Module):\n",
    "        \n",
    "    def __init__(self, dim, h=14, w=8):\n",
    "        super().__init__()\n",
    "        self.complex_weight = nn.Parameter(torch.randn(h, w, dim, 2, dtype=torch.float32) * 0.02)\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "\n",
    "    def forward(self, x, spatial_size=None):\n",
    "        B, N, C = x.shape\n",
    "        if spatial_size is None:\n",
    "            a = b = int(math.sqrt(N))\n",
    "        else:\n",
    "            a, b = spatial_size\n",
    "        x = x.view(B, a, b, C) \n",
    "        x = x.to(torch.float32)\n",
    "        x = torch.fft.rfft2(x, dim=(1, 2), norm='ortho')\n",
    "        weight = nn.Parameter(torch.randn(x.shape) * 0.02)\n",
    "        x = x * weight\n",
    "        x = torch.fft.irfft2(x, s=(a, b), dim=(1, 2), norm='ortho') # Dimensions will be trimmed to s or zero padded\n",
    "        x = x.reshape(B, N, C)\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "  def __init__(self, dim, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.fclayer1 = Linear(dim, mlp_dim)\n",
    "        self.fclayer2 = Linear(mlp_dim, dim)\n",
    "        self.act_fn = nn.GELU()\n",
    "        self.dropout = Dropout(0.1)\n",
    "        self._init_weights()\n",
    "\n",
    "  def _init_weights(self):\n",
    "\n",
    "      nn.init.xavier_uniform_(self.fclayer1.weight)\n",
    "      nn.init.xavier_uniform_(self.fclayer2.weight)\n",
    "\n",
    "      nn.init.normal_(self.fclayer1.bias, std=1e-6)\n",
    "      nn.init.normal_(self.fclayer2.bias, std=1e-6)\n",
    "\n",
    "  def forward(self, x):\n",
    "      x = self.fclayer1(x)\n",
    "      x = self.act_fn(x)\n",
    "      x = self.dropout(x)\n",
    "      x = self.fclayer2(x)\n",
    "      x = self.dropout(x)\n",
    "      return x\n",
    "  \n",
    "  \n",
    "  \n",
    "  \n",
    "class SingleEncoderBlock(nn.Module):\n",
    "    def __init__(self,dim, num_heads, mlp_dim):\n",
    "        super().__init__()\n",
    "        print(f\"Encoder started executing\")\n",
    "        self.layer_norm = LayerNorm(dim, eps=1e-6) # First LayerNorm layer\n",
    "\n",
    "        self.ffn_norm = LayerNorm(dim, eps=1e-6) # Second LayerNorm layer\n",
    "        # print(f\"second layer norm executed\")\n",
    "        self.ffn = MultiLayerPerceptron(dim, mlp_dim) # MLP layer\n",
    "        # print(f\"MLP executed\")\n",
    "        self.cross_hl_attention = CrossHL_attention(dim = dim, patches = 11**2) # Cross-HL Attention layer\n",
    "        self.Globalfilter = GlobalFilter(dim, h=14, w=8) # Global Filter layer\n",
    "    def forward(self, x1,x2):\n",
    "        res = x1\n",
    "        x = self.layer_norm(x1)\n",
    "        # print(f\"First layer norm executed\")\n",
    "        # print(f\"Before entering GFLayer shape is {x.shape}\")\n",
    "        x_gf=self.Globalfilter(x) # Global Filter after normalizing\n",
    "        # print(f\"Global Filter layer executed with shape as {x.shape}\")\n",
    "        # print(\"Now entering Cross HL layer\")\n",
    "        x_chl= self.cross_hl_attention(x,x2) #Cross attention after apply global filters on x1 (HSI)\n",
    "        print(f\"Cross attention layer executed with shape as {x.shape}\")\n",
    "        x_chl = x_chl + res\n",
    "        # x = x + res\n",
    "        res = x\n",
    "        x_chl = self.ffn_norm(x_chl)\n",
    "        x_gf= self.ffn_norm(x_gf)\n",
    "        x=torch.cat((x_chl, x_gf), dim=0)\n",
    "        x = self.ffn(x)\n",
    "        x = x + res\n",
    "        return x\n",
    "    \n",
    "    \n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, mlp_dim=512, depth=2):\n",
    "        super().__init__()\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = LayerNorm(dim, eps=1e-6)\n",
    "        for _ in range(depth):\n",
    "            layer = SingleEncoderBlock(dim, num_heads, mlp_dim)\n",
    "            self.layer.append(copy.deepcopy(layer))\n",
    "\n",
    "    def forward(self, x, x2):\n",
    "        for layer_block in self.layer:\n",
    "            x = layer_block(x, x2)\n",
    "        encoded = self.encoder_norm(x)\n",
    "        return encoded[:, 0]\n",
    "\n",
    "\n",
    "class CrossHL_Transformer(nn.Module):\n",
    "    def __init__(self, FM, NC, NCLidar, Classes, patchsize):\n",
    "        super(CrossHL_Transformer, self).__init__()\n",
    "        self.patchsize = patchsize\n",
    "        self.NCLidar = NCLidar\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv3d(1, 8, (9, 3, 3), padding=(0, 1, 1), stride=1),\n",
    "            nn.BatchNorm3d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.hetconv_layer = nn.Sequential(\n",
    "            HetConv(8 * (NC - 8), FM*4, p=1, g=(FM*4)//4 if (8 * (NC - 8))%FM == 0 else (FM*4)//8),\n",
    "            nn.BatchNorm2d(FM*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.ca = Encoder(FM*4)\n",
    "        self.fclayer = nn.Linear(FM*4, Classes)\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, (patchsize**2), FM*4))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        torch.nn.init.xavier_uniform_(self.fclayer.weight)\n",
    "        torch.nn.init.normal_(self.fclayer.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = x1.reshape(x1.shape[0], -1, self.patchsize, self.patchsize).unsqueeze(1).to(device)\n",
    "        x2 = x2.reshape(x1.shape[0], -1, self.patchsize*self.patchsize).to(device)\n",
    "        if x2.shape[1] > 0:\n",
    "            x2 = F.adaptive_avg_pool1d(x2.flatten(2).transpose(1, 2), 1).transpose(1, 2).reshape(x1.shape[0], -1, self.patchsize*self.patchsize)\n",
    "        x1 = self.conv5(x1)\n",
    "        x1 = x1.reshape(x1.shape[0], -1, self.patchsize, self.patchsize)\n",
    "        x1 = self.hetconv_layer(x1)\n",
    "        x1 = x1.flatten(2).transpose(-1, -2)\n",
    "        x = x1 + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        x = self.ca(x, x2)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        out = self.fclayer(x)\n",
    "        return out\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For GPU\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as dataf\n",
    "import time\n",
    "from torchsummary import summary\n",
    "from torch.nn import LayerNorm, Linear, Dropout, Softmax\n",
    "from torch.nn.modules.container import Sequential\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import torch.fft\n",
    "import math\n",
    "from functools import partial\n",
    "import logger\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class HetConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=None, bias=None, p=64, g=64):\n",
    "        super(HetConv, self).__init__()\n",
    "        self.groupwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, groups=g, padding=kernel_size//3, stride=stride)\n",
    "        self.pointwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, groups=p, stride=stride)\n",
    "    def forward(self, x):\n",
    "        return self.groupwise_conv(x) + self.pointwise_conv(x)\n",
    "\n",
    "class CrossHL_attention(nn.Module):\n",
    "    def __init__(self, dim, patches, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.1, proj_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.dim = dim\n",
    "        self.Wq = nn.Linear(patches, dim * num_heads, bias=qkv_bias)\n",
    "        self.Wk = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.Wv = nn.Linear(patches, dim, bias=qkv_bias)\n",
    "        self.linear_projection = nn.Linear(dim * num_heads, dim)\n",
    "        self.linear_projection_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x, x2):\n",
    "        B, N, C = x.shape\n",
    "        query = self.Wq(x2).reshape(B, self.num_heads, self.num_heads, self.dim // self.num_heads).permute(0, 1, 2, 3)\n",
    "        key = self.Wk(x).reshape(B, N, self.num_heads, self.dim // self.num_heads).permute(0, 2, 1, 3)\n",
    "        value = self.Wv(x.transpose(1,2)).reshape(B, C, self.num_heads, self.dim // self.num_heads).permute(0, 2, 3, 1)\n",
    "        attention = torch.einsum('bhid,bhjd->bhij', key, query) * self.scale\n",
    "        attention = attention.softmax(dim=-1)\n",
    "        x = torch.einsum('bhij,bhjd->bhid', attention, value)\n",
    "        x = x.reshape(B, N, -1)\n",
    "        x = self.linear_projection(x)\n",
    "        x = self.linear_projection_drop(x)\n",
    "        return x\n",
    "\n",
    "class GlobalFilter(nn.Module):\n",
    "    def __init__(self, dim, h=14, w=8):\n",
    "        super().__init__()\n",
    "        self.complex_weight = nn.Parameter(torch.randn(h, w, dim, 2, dtype=torch.float32) * 0.02)\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "\n",
    "    def forward(self, x, spatial_size=None):\n",
    "        B, N, C = x.shape\n",
    "        if spatial_size is None:\n",
    "            a = b = int(math.sqrt(N))\n",
    "        else:\n",
    "            a, b = spatial_size\n",
    "        x = x.view(B, a, b, C)\n",
    "        x = x.to(torch.float32)\n",
    "        x = torch.fft.rfft2(x, dim=(1, 2), norm='ortho')\n",
    "        weight = nn.Parameter(torch.randn(x.shape) * 0.02).to(device)\n",
    "        x = x * weight\n",
    "        x = torch.fft.irfft2(x, s=(a, b), dim=(1, 2), norm='ortho')\n",
    "        x = x.reshape(B, N, C)\n",
    "        return x\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.fclayer1 = Linear(dim, mlp_dim)\n",
    "        self.fclayer2 = Linear(mlp_dim, dim)\n",
    "        self.act_fn = nn.GELU()\n",
    "        self.dropout = Dropout(0.1)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fclayer1.weight)\n",
    "        nn.init.xavier_uniform_(self.fclayer2.weight)\n",
    "        nn.init.normal_(self.fclayer1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fclayer2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fclayer1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fclayer2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class SingleEncoderBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.attention_norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.ffn_norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.ffn = MultiLayerPerceptron(dim, mlp_dim)\n",
    "        self.cross_hl_attention = CrossHL_attention(dim=dim, patches=11**2)\n",
    "        self.Globalfilter = GlobalFilter(dim, h=14, w=8)\n",
    "    def forward(self, x1, x2):\n",
    "        res = x1\n",
    "        x = self.layer_norm(x1)\n",
    "        x_gf=self.Globalfilter(x) \n",
    "        x_chl= self.cross_hl_attention(x,x2)\n",
    "        print(f\"Cross attention layer executed with shape as {x.shape}\")\n",
    "        x_chl = x_chl + res\n",
    "        # x = x + res\n",
    "        res = x\n",
    "        x_chl = self.ffn_norm(x_chl)\n",
    "        x_gf= self.ffn_norm(x_gf)\n",
    "        x=torch.cat((x_chl, x_gf), dim=0)\n",
    "        x = self.ffn(x)\n",
    "        x = x + res\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, mlp_dim=512, depth=2):\n",
    "        super().__init__()\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = LayerNorm(dim, eps=1e-6)\n",
    "        for _ in range(depth):\n",
    "            layer = SingleEncoderBlock(dim, num_heads, mlp_dim)\n",
    "            self.layer.append(copy.deepcopy(layer))\n",
    "\n",
    "    def forward(self, x, x2):\n",
    "        for layer_block in self.layer:\n",
    "            x = layer_block(x, x2)\n",
    "        encoded = self.encoder_norm(x)\n",
    "        return encoded[:, 0]\n",
    "\n",
    "class CrossHL_Transformer(nn.Module):\n",
    "    def __init__(self, FM, NC, NCLidar, Classes, patchsize):\n",
    "        super(CrossHL_Transformer, self).__init__()\n",
    "        self.patchsize = patchsize\n",
    "        self.NCLidar = NCLidar\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv3d(1, 8, (9, 3, 3), padding=(0, 1, 1), stride=1),\n",
    "            nn.BatchNorm3d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.hetconv_layer = nn.Sequential(\n",
    "            HetConv(8 * (NC - 8), FM*4, p=1, g=(FM*4)//4 if (8 * (NC - 8))%FM == 0 else (FM*4)//8),\n",
    "            nn.BatchNorm2d(FM*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.ca = Encoder(FM*4)\n",
    "        self.fclayer = nn.Linear(FM*4, Classes)\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, (patchsize**2), FM*4))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        torch.nn.init.xavier_uniform_(self.fclayer.weight)\n",
    "        torch.nn.init.normal_(self.fclayer.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = x1.reshape(x1.shape[0], -1, self.patchsize, self.patchsize).unsqueeze(1).to(device)\n",
    "        x2 = x2.reshape(x1.shape[0], -1, self.patchsize*self.patchsize).to(device)\n",
    "        if x2.shape[1] > 0:\n",
    "            x2 = F.adaptive_avg_pool1d(x2.flatten(2).transpose(1, 2), 1).transpose(1, 2).reshape(x1.shape[0], -1, self.patchsize*self.patchsize)\n",
    "        x1 = self.conv5(x1)\n",
    "        x1 = x1.reshape(x1.shape[0], -1, self.patchsize, self.patchsize)\n",
    "        x1 = self.hetconv_layer(x1)\n",
    "        x1 = x1.flatten(2).transpose(-1, -2)\n",
    "        x = x1 + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        x = self.ca(x, x2)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        out = self.fclayer(x)\n",
    "        return out\n",
    "\n",
    "# Dataset and other parts of the code where data is loaded and moved to GPU should be updated accordingly.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## For GPU\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.utils.data as dataf\n",
    "import time\n",
    "from torchsummary import summary\n",
    "from torch.nn import LayerNorm, Linear, Dropout, Softmax\n",
    "from torch.nn.modules.container import Sequential\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "import torch.fft\n",
    "import math\n",
    "from functools import partial\n",
    "import logger\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "class HetConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size=3, stride=1, padding=None, bias=None, p=64, g=64):\n",
    "        super(HetConv, self).__init__()\n",
    "        self.groupwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size=kernel_size, groups=g, padding=kernel_size//3, stride=stride)\n",
    "        self.pointwise_conv = nn.Conv2d(in_channels, out_channels, kernel_size=1, groups=p, stride=stride)\n",
    "    def forward(self, x):\n",
    "        return self.groupwise_conv(x) + self.pointwise_conv(x)\n",
    "\n",
    "class CrossHL_attention(nn.Module):\n",
    "    def __init__(self, dim, patches, num_heads=8, qkv_bias=False, qk_scale=None, attn_drop=0.1, proj_drop=0.1):\n",
    "        super().__init__()\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = qk_scale or head_dim ** -0.5\n",
    "        self.dim = dim\n",
    "        self.Wq = nn.Linear(patches, dim * num_heads, bias=qkv_bias)\n",
    "        self.Wk = nn.Linear(dim, dim, bias=qkv_bias)\n",
    "        self.Wv = nn.Linear(patches, dim, bias=qkv_bias)\n",
    "        self.linear_projection = nn.Linear(dim * num_heads, dim)\n",
    "        self.linear_projection_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x, x2):\n",
    "        B, N, C = x.shape\n",
    "        query = self.Wq(x2).reshape(B, self.num_heads, self.num_heads, self.dim // self.num_heads).permute(0, 1, 2, 3)\n",
    "        key = self.Wk(x).reshape(B, N, self.num_heads, self.dim // self.num_heads).permute(0, 2, 1, 3)\n",
    "        value = self.Wv(x.transpose(1,2)).reshape(B, C, self.num_heads, self.dim // self.num_heads).permute(0, 2, 3, 1)\n",
    "        attention = torch.einsum('bhid,bhjd->bhij', key, query) * self.scale\n",
    "        attention = attention.softmax(dim=-1)\n",
    "        x = torch.einsum('bhij,bhjd->bhid', attention, value)\n",
    "        x = x.reshape(B, N, -1)\n",
    "        x = self.linear_projection(x)\n",
    "        x = self.linear_projection_drop(x)\n",
    "        return x\n",
    "\n",
    "class GlobalFilter(nn.Module):\n",
    "    def __init__(self, dim, h=14, w=8):\n",
    "        super().__init__()\n",
    "        self.complex_weight = nn.Parameter(torch.randn(h, w, dim, 2, dtype=torch.float32) * 0.02)\n",
    "        self.w = w\n",
    "        self.h = h\n",
    "\n",
    "    def forward(self, x, spatial_size=None):\n",
    "        B, N, C = x.shape\n",
    "        if spatial_size is None:\n",
    "            a = b = int(math.sqrt(N))\n",
    "        else:\n",
    "            a, b = spatial_size\n",
    "        x = x.view(B, a, b, C)\n",
    "        x = x.to(torch.float32)\n",
    "        x = torch.fft.rfft2(x, dim=(1, 2), norm='ortho')\n",
    "        weight = nn.Parameter(torch.randn(x.shape) * 0.02).to(device)\n",
    "        x = x * weight\n",
    "        x = torch.fft.irfft2(x, s=(a, b), dim=(1, 2), norm='ortho')\n",
    "        x = x.reshape(B, N, C)\n",
    "        return x\n",
    "\n",
    "class MultiLayerPerceptron(nn.Module):\n",
    "    def __init__(self, dim, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.fclayer1 = Linear(dim, mlp_dim)\n",
    "        self.fclayer2 = Linear(mlp_dim, dim)\n",
    "        self.act_fn = nn.GELU()\n",
    "        self.dropout = Dropout(0.1)\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        nn.init.xavier_uniform_(self.fclayer1.weight)\n",
    "        nn.init.xavier_uniform_(self.fclayer2.weight)\n",
    "        nn.init.normal_(self.fclayer1.bias, std=1e-6)\n",
    "        nn.init.normal_(self.fclayer2.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.fclayer1(x)\n",
    "        x = self.act_fn(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fclayer2(x)\n",
    "        x = self.dropout(x)\n",
    "        return x\n",
    "\n",
    "class SingleEncoderBlock(nn.Module):\n",
    "    def __init__(self, dim, num_heads, mlp_dim):\n",
    "        super().__init__()\n",
    "        self.attention_norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.ffn_norm = LayerNorm(dim, eps=1e-6)\n",
    "        self.ffn = MultiLayerPerceptron(dim, mlp_dim)\n",
    "        self.cross_hl_attention = CrossHL_attention(dim=dim, patches=11**2)\n",
    "        self.Globalfilter = GlobalFilter(dim, h=14, w=8)\n",
    "    def forward(self, x1, x2):\n",
    "        res = x1\n",
    "        x = self.attention_norm(x1)\n",
    "        x_gf=self.Globalfilter(x) \n",
    "        x_chl= self.cross_hl_attention(x,x2)\n",
    "        print(f\"Cross attention layer executed with shape as {x.shape}\")\n",
    "        x_chl = x_chl + res\n",
    "        # x = x + res\n",
    "        res = x\n",
    "        x_chl = self.ffn_norm(x_chl)\n",
    "        x_gf= self.ffn_norm(x_gf)\n",
    "        x=torch.cat((x_chl, x_gf), dim=0)\n",
    "        x = self.ffn(x)\n",
    "        x = x + res\n",
    "        return x\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, dim, num_heads=8, mlp_dim=512, depth=2):\n",
    "        super().__init__()\n",
    "        self.layer = nn.ModuleList()\n",
    "        self.encoder_norm = LayerNorm(dim, eps=1e-6)\n",
    "        for _ in range(depth):\n",
    "            layer = SingleEncoderBlock(dim, num_heads, mlp_dim)\n",
    "            self.layer.append(copy.deepcopy(layer))\n",
    "\n",
    "    def forward(self, x, x2):\n",
    "        for layer_block in self.layer:\n",
    "            x = layer_block(x, x2)\n",
    "        encoded = self.encoder_norm(x)\n",
    "        return encoded[:, 0]\n",
    "\n",
    "class CrossHL_Transformer(nn.Module):\n",
    "    def __init__(self, FM, NC, NCLidar, Classes, patchsize):\n",
    "        super(CrossHL_Transformer, self).__init__()\n",
    "        self.patchsize = patchsize\n",
    "        self.NCLidar = NCLidar\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv3d(1, 8, (9, 3, 3), padding=(0, 1, 1), stride=1),\n",
    "            nn.BatchNorm3d(8),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.hetconv_layer = nn.Sequential(\n",
    "            HetConv(8 * (NC - 8), FM*4, p=1, g=(FM*4)//4 if (8 * (NC - 8))%FM == 0 else (FM*4)//8),\n",
    "            nn.BatchNorm2d(FM*4),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.ca = Encoder(FM*4)\n",
    "        self.fclayer = nn.Linear(FM*4, Classes)\n",
    "        self.position_embeddings = nn.Parameter(torch.randn(1, (patchsize**2), FM*4))\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        torch.nn.init.xavier_uniform_(self.fclayer.weight)\n",
    "        torch.nn.init.normal_(self.fclayer.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x1, x2):\n",
    "        x1 = x1.reshape(x1.shape[0], -1, self.patchsize, self.patchsize).unsqueeze(1).to(device)\n",
    "        x2 = x2.reshape(x1.shape[0], -1, self.patchsize*self.patchsize).to(device)\n",
    "        if x2.shape[1] > 0:\n",
    "            x2 = F.adaptive_avg_pool1d(x2.flatten(2).transpose(1, 2), 1).transpose(1, 2).reshape(x1.shape[0], -1, self.patchsize*self.patchsize)\n",
    "        x1 = self.conv5(x1)\n",
    "        x1 = x1.reshape(x1.shape[0], -1, self.patchsize, self.patchsize)\n",
    "        x1 = self.hetconv_layer(x1)\n",
    "        x1 = x1.flatten(2).transpose(-1, -2)\n",
    "        x = x1 + self.position_embeddings\n",
    "        x = self.dropout(x)\n",
    "        x = self.ca(x, x2)\n",
    "        x = x.reshape(x.shape[0], -1)\n",
    "        out = self.fclayer(x)\n",
    "        return out\n",
    "\n",
    "# Dataset and other parts of the code where data is loaded and moved to GPU should be updated accordingly.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
